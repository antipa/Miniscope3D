{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "%load_ext autoreload   \n",
    "%autoreload 2\n",
    "\n",
    "#Autoreloading of modules so you don't have to restart kernel after editing .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tf.enable_eager_execution()\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from miniscope_utils_pos_tf import *\n",
    "#import utils as krist\n",
    "import scipy.misc as sc\n",
    "import scipy.special as scsp\n",
    "from skimage.transform import resize as imresize\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "from IPython import display\n",
    "import scipy.ndimage as ndim\n",
    "import scipy.misc as misc\n",
    "from scipy import signal\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.animation as animation\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import time\n",
    "from itertools import permutations\n",
    "from itertools import combinations\n",
    "#import copy\n",
    "#from bridson import poisson_disc_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#device = '/gpu:0'\n",
    "#print(tf.test.is_gpu_available())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ho=1 #microns\n",
    "lam=0.510\n",
    "l=1\n",
    "n1=1.51\n",
    "n2=1\n",
    "phi=np.linspace(0,2*np.pi,100)\n",
    "H=Ho+lam*phi*l/(2*np.pi*0.51)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model and loss\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, zsampling = 'uniform_random', cross_corr_norm = 'log_sum_exp'):   #'log_sum_exp'\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.samples = (768,768)   #Grid for PSF simulation\n",
    "\n",
    "        # min and max lenslet focal lengths in mm\n",
    "        self.fmin = 6.\n",
    "        self.fmax = 20.\n",
    "        self.ior = 1.56\n",
    "        self.lam=510e-6\n",
    "        \n",
    "        # Min and max lenslet radii\n",
    "        self.Rmin = self.fmin*(self.ior-1.)\n",
    "        self.Rmax = self.fmax*(self.ior-1.)\n",
    "\n",
    "        # Convert to curvatures\n",
    "        self.cmin = 1/self.Rmax\n",
    "        self.cmax = 1/self.Rmin\n",
    "        self.xgrng = np.array((-1.8, 1.8)).astype('float32')    #Range, in mm, of grid of the whole plane (not just grin)\n",
    "        self.ygrng = np.array((-1.8, 1.8)).astype('float32')\n",
    "\n",
    "        self.t = 10.    #Distance to sensor from mask in mm\n",
    "\n",
    "        #Compute depth range of virtual image that mask sees (this is assuming an objective is doing some magnification)\n",
    "\n",
    "        self.zmin_virtual = 1./(1./self.t - 1./self.fmin)\n",
    "        self.zmax_virtual = 1./(1./self.t - 1./self.fmax)\n",
    "        self.CA = .9; #semi clear aperature of GRIN\n",
    "        self.mean_lenslet_CA = .21 #average lenslest semi clear aperture in mm. \n",
    "            \n",
    "        #Getting number of lenslets and z planes needed as well as defocus list\n",
    "        self.ps = (self.xgrng[1] - self.xgrng[0])/self.samples[0]\n",
    "        self.Nlenslets=np.int(np.floor((self.CA**2)/(self.mean_lenslet_CA**2)))\n",
    "        #self.Nz = np.ceil(np.sqrt(self.Nlenslets*2)).astype('int') #number of Zplanes \n",
    "        self.Nz=7\n",
    "        self.zsampling = zsampling\n",
    "        self.grid_z_planes=20\n",
    "        #self.defocus_grid=  1./(np.linspace(1/self.zmin_virtual, 1./self.zmax_virtual, self.grid_z_planes)) #mm or dioptres\n",
    "\n",
    "        #if self.zsampling is 'fixed':\n",
    "        #    self.defocus_list = 1./(np.linspace(1/self.zmin_virtual, 1./self.zmax_virtual, self.Nz)) #mm or dioptres\n",
    "            \n",
    "        self.min_offset= -10e-3\n",
    "        self.max_offset= 10e-3\n",
    "        self.lenslet_offset=tfe.Variable(tf.zeros(self.Nlenslets),name='offset', dtype = tf.float32,constraint=lambda t: tf.clip_by_value(t,self.min_offset, self.max_offset))\n",
    "        #initializing the x and y positions\n",
    "        [xpos,ypos, rlist]=poissonsampling_circular(self)\n",
    "        \n",
    "        self.min_r= self.Rmin\n",
    "        self.max_r= self.Rmax\n",
    "        self.rlist = tfe.Variable(rlist,name='rlist', dtype = tf.float32,constraint=lambda t: tf.clip_by_value(t,self.min_r, self.max_r))\n",
    "        #self.xpos = tfe.Variable(xpos, name='xpos', dtype = tf.float32, constraint=lambda t: tf.clip_by_value(t,-self.CA, self.CA))\n",
    "        #self.ypos = tfe.Variable(ypos, name='ypos', dtype = tf.float32, constraint=lambda t: tf.clip_by_value(t,-self.CA, self.CA))\n",
    "        self.xpos = tfe.Variable(xpos, name='xpos', dtype = tf.float32)\n",
    "        self.ypos = tfe.Variable(ypos, name='ypos', dtype = tf.float32)\n",
    "        #parameters for making the lenslet surface\n",
    "        self.yg = tf.constant(np.linspace(self.ygrng[0], self.ygrng[1], self.samples[0]),dtype=tf.float32)\n",
    "        self.xg=tf.constant(np.linspace(self.xgrng[0], self.xgrng[1], self.samples[1]),dtype=tf.float32)\n",
    "        self.px=tf.constant(self.xg[1] - self.xg[0],tf.float32)\n",
    "        self.py=tf.constant(self.yg[1] - self.yg[0],tf.float32)\n",
    "        self.xgm, self.ygm = tf.meshgrid(self.xg,self.yg)\n",
    "\n",
    "        #PSF generation parameters\n",
    "        self.lam=tf.constant(510e-6,dtype=tf.float32)\n",
    "        self.k = np.pi*2./self.lam\n",
    "        \n",
    "        fx = tf.constant(np.linspace(-1./(2.*self.ps),1./(2.*self.ps),self.samples[1]),dtype=tf.float32)\n",
    "        fy = tf.constant(np.linspace(-1./(2.*self.ps),1./(2.*self.ps),self.samples[0]),dtype=tf.float32)\n",
    "        self.Fx,self.Fy = tf.meshgrid(fx,fy)\n",
    "        self.field_list = tf.constant(np.array((0., 0.)).astype('float32'))\n",
    "        target_option='airy'\n",
    "        self.corr_pad_frac = 0.5\n",
    "        self.target_res = 2.5e-3 # mm \n",
    "        \n",
    "        \n",
    "        \n",
    "        D=1.22*self.lam/(tf.sin(2*tf.atan(self.target_res/(2*self.t))))  #0.514 for half_max, 1.22 for first zero. \n",
    "        x_airy=self.k*D*tf.sin(tf.atan(self.xgm/self.t))/2\n",
    "        y_airy=self.k*D*tf.sin(tf.atan(self.ygm/self.t))/2\n",
    "        Ra = tf.sqrt(tf.square(x_airy) + tf.square(y_airy))\n",
    "        self.target_airy = (2*scsp.j1(Ra)/Ra)**2\n",
    "        self.target_airy_pad = pad_frac_tf(self.target_airy / tf.reduce_max(self.target_airy), self.corr_pad_frac)\n",
    "       \n",
    "    \n",
    "        \n",
    "\n",
    "        #         self.target_airy=((2*scsp.j1(x_airy)/x_airy)**2) *((2*scsp.j1(y_airy)/y_airy)**2)\n",
    "        \n",
    "        sig_aprox=0.42*self.lam*self.t/D\n",
    "        self.airy_aprox_target = tf.exp(-(tf.square(self.xgm) + tf.square(self.ygm))/(2*tf.square(sig_aprox)))\n",
    "        self.airy_aprox_pad = pad_frac_tf(self.airy_aprox_target / tf.reduce_max(self.airy_aprox_target), self.corr_pad_frac)\n",
    "    \n",
    "        \n",
    "        sig = 2*self.target_res/(2.35482)\n",
    "        self.real_target = tf.exp(-(tf.square(self.xgm) + tf.square(self.ygm))/(2*tf.square(sig)))\n",
    "        self.real_target_pad = pad_frac_tf(self.real_target / tf.reduce_max(self.real_target), self.corr_pad_frac)\n",
    "        if target_option=='airy':\n",
    "            self.target_F = tf.abs(tf.fft2d(tf.complex(tf_fftshift(self.target_airy_pad), 0.)))\n",
    "        elif target_option=='airy_aprox':\n",
    "            self.target_F = tf.abs(tf.fft2d(tf.complex(tf_fftshift(self.airy_aprox_pad), 0.)))\n",
    "                \n",
    "        else:\n",
    "            self.target_F = tf.abs(tf.fft2d(tf.complex(tf_fftshift(self.real_target_pad), 0.)))\n",
    "        \n",
    "        \n",
    "        # Set regularization. Problem will be treated as l1 of spectral error at each depth + tau * l_inf(cross_corr)\n",
    "        self.cross_corr_norm = cross_corr_norm\n",
    "        self.logsumexp_param = tf.constant(1e-3, tf.float32)   #lower is better l-infinity approximation, higher is worse but smoother\n",
    "        self.tau = tf.constant(1,tf.float32)    #Extra weight for cross correlation terms\n",
    "        \n",
    "        \n",
    "        self.ignore_dc = True   #Ignore DC in autocorrelations when computing frequency domain loss\n",
    "        dc_mask = np.ones_like(self.target_F.numpy())  #Mask for DC. Set to zero anywhere we want to ignore loss computation of power spectrum\n",
    "        dc_mask[:3,:3] = 0\n",
    "        dc_mask[-2:,:1] = 0\n",
    "        dc_mask[:1,-2:] = 0\n",
    "        dc_mask[-2:,-2:]= 0\n",
    "        self.dc_mask = tf.constant(dc_mask,tf.float32)\n",
    "        \n",
    "        \n",
    "    def call(self, defocus_list):\n",
    "\n",
    "        if np.size(defocus_list) == 1:\n",
    "            defocus_list = 1./(np.linspace(1/self.zmin_virtual, 1./self.zmax_virtual, self.Nz)) #mm or dioptres\n",
    "            \n",
    "            #if self.zsampling is \"uniform_random\":\n",
    "            #    self.defocus_list = 1/np.sort(np.random.uniform(low = 1/self.zmin_virtual, high = 1./self.zmax_virtual, size = (self.Nz,)))\n",
    "            #\n",
    "            #elif self.zsampling is \"random_grid\":\n",
    "            #    testint = random.sample(range(0,model.grid_z_planes), model.Nz)\n",
    "            #    self.defocus_list = self.defocus_grid[testint]\n",
    "        #else:\n",
    "        #    self.defocus_list = grid_opt\n",
    "            \n",
    "        T,aper=make_lenslet_tf(self) #offset added\n",
    "        # Get psf stack\n",
    "        zstack = self.gen_psf_stack(T, aper, 0.5, defocus_list)\n",
    "        \n",
    "        psf_spect = self.gen_stack_spectrum(zstack)\n",
    "\n",
    "        normsize=tf.to_float(tf.shape(psf_spect)[0]*tf.shape(psf_spect)[1])\n",
    "        \n",
    "        Rmat_tf_diag = []\n",
    "        Rmat_tf_off_diag = []\n",
    "        #calculating Xcorr\n",
    "        \n",
    "        \n",
    "        # This now computes diagonals and off-diagonals separately then concatenates them. this makes is easier to \"find\" the diagonals/off diagonals for separate treatment  later.\n",
    "        for z1 in range(self.Nz):\n",
    "            for z2 in range(z1, self.Nz):\n",
    "                Fcorr = tf.conj(psf_spect[z1])*psf_spect[z2]\n",
    "                if z1 != z2:\n",
    "                    # Target is zero for cross correlation\n",
    "                    if self.cross_corr_norm is 'l2':\n",
    "                        Rmat_tf_off_diag.append( self.tau * tf.reduce_sum(tf.square(tf.abs(Fcorr)/normsize)))  #changed to one norm\n",
    "                    elif self.cross_corr_norm is 'log_sum_exp':   \n",
    "                        # Implementation of eq. 7 from http://users.cecs.anu.edu.au/~yuchao/files/SmoothApproximationforL-infinityNorm.pdf\n",
    "                        ccorr = tf.abs(tf.ifft2d(Fcorr))\n",
    "                        Rmat_tf_off_diag.append(2 * self.tau * self.logsumexp_param * tf.reduce_logsumexp(tf.square(ccorr)/self.logsumexp_param) )\n",
    "                        \n",
    "                    elif self.cross_corr_norm is 'inf':\n",
    "                        Rmat_tf_off_diag.append( self.tau * tf.reduce_max(tf.abs(tf.ifft2d(Fcorr))))\n",
    "        Rmat_tf = tf.concat([Rmat_tf_diag, Rmat_tf_off_diag],0)\n",
    "                        \n",
    "                        \n",
    "\n",
    "        #Rmat=tf.reshape(Rmat_tf,(self.Nz,self.Nz))\n",
    "            \n",
    "        return Rmat_tf #note this returns int data type!! vector not matrix. This is also my loss!\n",
    "    \n",
    "    def gen_correlation_stack(self,psf_spect):\n",
    "        Fcorr=[]\n",
    "        for z1 in range(self.Nz):\n",
    "            for z2 in range(self.Nz):\n",
    "                Fcorr.append(tf_fftshift(tf.ifft2d(tf.conj(psf_spect[z1])*psf_spect[z2])))\n",
    "\n",
    "        return Fcorr\n",
    "        \n",
    "    def gen_psf_stack(self, T, aper, prop_pad, zplanes=0):\n",
    "        zstack = []\n",
    "        \n",
    "        if np.size(zplanes) == 1:\n",
    "            zplanes = 1./(np.linspace(1/self.zmin_virtual, 1./self.zmax_virtual, self.Nz)) #mm or dioptres\n",
    "            \n",
    "        #if np.size(zplanes_opt) == 0:\n",
    "        #    zplanes = self.defocus_list\n",
    "        #else:\n",
    "        #    zplanes = zplanes_opt\n",
    "            \n",
    "        for defocus in zplanes:\n",
    "            zstack.append(gen_psf_ag_tf(T,self,defocus,'angle',0., prop_pad))\n",
    "        return zstack\n",
    "    \n",
    "    def gen_stack_spectrum(self, zstack):\n",
    "                #Padding for fft\n",
    "        psf_pad=[]\n",
    "#         Rmat = np.zeros((self.Nz,self.Nz))\n",
    "\n",
    "        for z1 in range(self.Nz):\n",
    "            psf_pad.append(pad_frac_tf(zstack[z1],self.corr_pad_frac)) #how much to pad? \n",
    "\n",
    "        psf_spect=[]\n",
    "        \n",
    "        #Getting spectrum\n",
    "\n",
    "        for z1 in range(self.Nz):\n",
    "            psf_spect.append(tf.fft2d(tf.complex(tf_fftshift(psf_pad[z1]),tf.constant(0.,dtype = tf.float32))))\n",
    "\n",
    "        return psf_spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def distances(model, x,y):\n",
    "    dist = []\n",
    "    dist_bool = []\n",
    "    things = np.arange(model.Nlenslets)\n",
    "    test_perm = list(permutations(things, 2))\n",
    "\n",
    "    for i in range(0, len(test_perm)):\n",
    "        dist_i = tf.sqrt(tf.square(x[test_perm[i][0]]-x[test_perm[i][1]])+tf.square(y[test_perm[i][0]]-y[test_perm[i][1]]))\n",
    "        dist.append(dist_i)\n",
    "        dist_bool.append(dist_i>0*model.mean_lenslet_CA)  ##fix later\n",
    "        \n",
    "    return dist, dist_bool, test_perm\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(zsampling = 'random_grid')  # zsampling options: 'fixed' or 'uniform_random'\n",
    "Rmat=model(0)\n",
    "\n",
    "# Save initial values for later comparison \n",
    "R_init = Rmat\n",
    "Tinit,_=make_lenslet_tf(model)\n",
    "xinit = tf.Variable(tf.zeros(model.Nlenslets))\n",
    "tf.assign(xinit, model.xpos)\n",
    "\n",
    "\n",
    "yinit = tf.Variable(tf.zeros(model.Nlenslets))\n",
    "tf.assign(yinit, model.ypos)\n",
    "\n",
    "rinit = tf.Variable(tf.zeros(model.Nlenslets))\n",
    "tf.assign(rinit, model.rlist)\n",
    "\n",
    "offsetinit = tf.Variable(tf.zeros(model.Nlenslets))\n",
    "tf.assign(offsetinit, model.lenslet_offset)\n",
    "\n",
    "model_init=Model()\n",
    "Rmat_init = model_init(0)\n",
    "tf.assign(model_init.lenslet_offset, offsetinit)\n",
    "tf.assign(model_init.xpos, xinit)\n",
    "tf.assign(model_init.ypos, yinit)\n",
    "tf.assign(model_init.rlist, rinit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Tinit.numpy())\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(R_init.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dc_mask = np.ones_like(model.target_F.numpy())  #Mask for DC. Set to zero anywhere we want to ignore loss computation of power spectrum\n",
    "# dc_mask[:2,:2] = 0\n",
    "# dc_mask[-1,0] = 0\n",
    "# dc_mask[0,-1] = 0\n",
    "# dc_mask[-1,-1] = 0\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(dc_mask)\n",
    "# plt.colorbar()\n",
    "model.px*1000*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.load_weights(\"C:\\\\Users\\\\herbtipa\\\\lenslets_one_per_depth\")\n",
    "#print(model.dc_mask[-1,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, .5)\n",
    "test = model.gen_stack_spectrum(test2)\n",
    "test3=model.gen_correlation_stack(test)\n",
    "#ax[0].imshow(T)\n",
    "#plt.plot((model.dc_mask * tf.abs(test[1]))[-1,:200].numpy())\n",
    "#plt.plot((model.dc_mask * model.target_F)[-1,:200].numpy())\n",
    "for z in range(10):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    plt.cla()\n",
    "    plt.plot(np.abs(test3[z][model.samples[0],:]))\n",
    "    \n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.pause(.5)\n",
    "    \n",
    "#     #ax[1].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z])))), vmin = 0, vmax = 50)\n",
    "#     #ax[2].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z]))) - model.target_F), vmin = -50, vmax = 50)\n",
    "#     plt.cla()\n",
    "#     #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))), vmax = 50)\n",
    "#     #plt.imshow(tf_fftshift(np.abs(test[z])))\n",
    "#     plt.plot(tf_fftshift(tf.abs(test[z]))[768,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to return  maximum cross correlation\n",
    "def calc_max_xcorr(model):\n",
    "    \n",
    "    T,aper=make_lenslet_tf(model)\n",
    "    #model = Model()\n",
    "    psf_stack = model.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum = model.gen_stack_spectrum(psf_stack)\n",
    "    ccor_mx = []\n",
    "    for zref in range(model.Nz):\n",
    "        for z in range(model.Nz):\n",
    "            xcorr12 = tf.real(tf_fftshift(tf.ifft2d(stack_spectrum[zref]* tf.conj(stack_spectrum[z]))))\n",
    "            if z != zref:\n",
    "                ccor_mx.append(tf.reduce_max(xcorr12)) \n",
    "                    \n",
    "    return ccor_mx\n",
    "\n",
    "def re_init_model():\n",
    "    model.xpos.assign(xinit)\n",
    "    model.ypos.assign(yinit)\n",
    "\n",
    "    model.lenslet_offset.assign(offsetinit)\n",
    "    model.rlist.assign(rinit)\n",
    "\n",
    "#have tf do everything for us\n",
    "def loss (model, inputs):\n",
    "    Rmat = model(inputs)\n",
    "    return tf.reduce_sum(tf.square(Rmat)), Rmat\n",
    "\n",
    "def loss_sum(model, inputs):\n",
    "    Rmat = model(inputs)\n",
    "    return tf.reduce_sum(Rmat), Rmat\n",
    "\n",
    "def loss_inf(model, inputs):\n",
    "    Rmat = model(inputs)\n",
    "    return tf.reduce_max(Rmat), Rmat\n",
    "\n",
    "def loss_mixed(model, inputs):\n",
    "    # max of off diags, sum of diags\n",
    "    Rmat = model(inputs)\n",
    "    diag_vec = Rmat[0:model.Nz]\n",
    "    off_diag = Rmat[model.Nz+1:-1]\n",
    "    return tf.reduce_sum(tf.abs(diag_vec)) + tf.reduce_max(off_diag), Rmat\n",
    "\n",
    "def gradient (model, myloss, inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        lossvalue, Rmat = myloss(model, inputs)\n",
    "        return tape.gradient(lossvalue, model.variables),lossvalue, Rmat\n",
    "    \n",
    "def gradients_and_scaling(model, loss, inputs):\n",
    "    grad,lossvalue, Rmat=gradient(model,loss, inputs)\n",
    "    grad[1] = grad[1] * 20000\n",
    "    grad[2] = grad[2] * 5000\n",
    "    grad[3] = grad[3] * 5000\n",
    "    #order=tf.reduce_max((tf.reduce_mean(grad[0]),tf.reduce_mean(grad[1]),tf.reduce_mean(grad[2]),tf.reduce_mean(grad[3])))\n",
    "    #grad[0]=grad[0]*(order/tf.reduce_mean(grad[0]))\n",
    "    #grad[1] = grad[1] * (order/tf.reduce_mean(grad[1]))\n",
    "    #grad[2]=grad[2]* (order/tf.reduce_mean(grad[2]))\n",
    "    #grad[3]=grad[3]*(order/tf.reduce_mean(grad[3]))\n",
    "    grads=remove_nan_gradients(grad)\n",
    "    return grads, lossvalue, Rmat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def averaged_gradient(model, loss, num_averages = 10):\n",
    "\n",
    "    grad_averaged, lossvalue, Rmat = gradients_and_scaling(model, loss)  # initial value \n",
    "    batch_loss = lossvalue\n",
    "    Rmat_avg = Rmat\n",
    "    \n",
    "    for g in range(0, num_averages-1):\n",
    "        grads, lossvalue, Rmat = gradients_and_scaling(model, loss)\n",
    "        batch_loss = batch_loss + lossvalue\n",
    "        Rmat_avg = Rmat_avg + Rmat\n",
    "        \n",
    "        \n",
    "        for grad_ind in range(0,len(model.variables)):\n",
    "            grad_averaged[grad_ind] = grad_averaged[grad_ind] + grads[grad_ind]\n",
    "\n",
    "\n",
    "    for grad_ind2 in range(0,len(model.variables)):\n",
    "            grad_averaged[grad_ind2] = grad_averaged[grad_ind2]*1/num_averages\n",
    "\n",
    "    batch_loss = batch_loss / num_averages\n",
    "    Rmat_avg = Rmat_avg/num_averages\n",
    "    \n",
    "    return grad_averaged, batch_loss, Rmat_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def constrain_distances(model, new_xpos, new_ypos, grads):\n",
    "    test_dist, test_dist_bool, perm = distances(model, new_xpos, new_ypos)\n",
    "    grads = np.ones((2,model.Nlenslets))\n",
    "    for i in range(0,len(perm)):\n",
    "        if test_dist_bool[i].numpy() == False:\n",
    "            index = perm[i][0]\n",
    "            grads[0,index] = 0\n",
    "            grads[1,index] = 0\n",
    "    return grads, test_dist, test_dist_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Options:\n",
    "step_size = 1e-11   #1e-8 works well for l2\n",
    "use_averaged_gradient = False  # True: uses averaged gradient, False: uses single gradient \n",
    "optimizer_type = 'gd'           # options: 'gd': normal gradient descent, 'nesterov': nesterov acceleration\n",
    "num_iterations = 100\n",
    "num_batches = 15\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=step_size,momentum= 0.9, use_nesterov = True)\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "losslist=[]\n",
    "rmean=[]\n",
    "\n",
    "defocus_grid=  1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, model.Nz * num_batches)) #mm or dioptres\n",
    "    \n",
    "\n",
    "if optimizer_type == 'nesterov':\n",
    "    tk = tf.constant(1,tf.float32)\n",
    "    tkp = tf.constant(1,tf.float32)\n",
    "\n",
    "    #xkp = model.variables\n",
    "    nvars = np.shape(model.variables)[0]\n",
    "    xk = []\n",
    "    xkp = []\n",
    "    [xk.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [xkp.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [tf.assign(xkp[n],model.variables[n]) for n in range(nvars)]\n",
    "\n",
    "    \n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    defocus_epoch = np.random.permutation(defocus_grid)\n",
    "    for j in range(0, num_batches):\n",
    "        if use_averaged_gradient == True:\n",
    "            grad,lossvalue, Rmat =  averaged_gradient(model, loss, num_averages = 10)\n",
    "        else: \n",
    "            grad, lossvalue, Rmat = gradients_and_scaling(model, loss, defocus_epoch[j*model.Nz:j*model.Nz+model.Nz])  # initial value \n",
    "\n",
    "\n",
    "        #grad,lossvalue, Rmat=gradient(model,loss)\n",
    "\n",
    "       # new_xpos = model.xpos - step_size*grad[2]\n",
    "       # new_ypos = model.xpos - step_size*grad[3] \n",
    "        #new_grad, test_dist, test_dist_bool = constrain_distances(model, new_xpos, new_ypos, grad) # apply constraint \n",
    "\n",
    "\n",
    "        # Gradient step\n",
    "        if optimizer_type == 'gd':\n",
    "            optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "            # Projection step\n",
    "            project_to_aper_keras(model)\n",
    "\n",
    "        if optimizer_type == 'nesterov':\n",
    "\n",
    "            optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\n",
    "            # Projection step\n",
    "            project_to_aper_keras(model)\n",
    "\n",
    "            # Update variables for next loop\n",
    "            tk = tkp\n",
    "            [tf.assign(xk[n],xkp[n]) for n in range(nvars)]\n",
    "\n",
    "            # Get state after project (gradient_step(yk))\n",
    "            [tf.assign(xkp[n],model.variables[n]) for n in range(nvars)]\n",
    "\n",
    "            #Acceleration\n",
    "            tkp = 0.5*(1.0 + tf.sqrt(1.0 + 4*tf.square(tk)))\n",
    "\n",
    "            bkp = (tk - 1)/tkp\n",
    "            ykp = [xkp[n] + bkp*(xkp[n] - xk[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "            # Update model variables (yk)\n",
    "            [model.variables[n].assign(ykp[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        T,aper=make_lenslet_tf(model)\n",
    "\n",
    "        losslist.append(lossvalue)\n",
    "\n",
    "        # Plotting Everything \n",
    "        plt.subplot(1,4,1)\n",
    "        plt.cla()\n",
    "        plt.imshow(T.numpy())\n",
    "        plt.title('surface')\n",
    "\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.cla()\n",
    "        plt.semilogy(losslist)\n",
    "        plt.title('loss')\n",
    "\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.cla()\n",
    "        pl = plt.plot(Rmat.numpy(),'k.')\n",
    "        plt.title('Rmat')\n",
    "\n",
    "        plt.subplot(1,4,4)\n",
    "        #plt.cla()\n",
    "        pl = plt.plot(model.rlist.numpy(),'k.')\n",
    "        plt.title('radii')\n",
    "\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        pl.clear()\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "#     pl.remove?\n",
    "    \n",
    "\n",
    "\n",
    "#cbar = fig.colorbar(rshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options:\n",
    "step_size = 1e-9   #1e-8 works well for l2\n",
    "use_averaged_gradient = False  # True: uses averaged gradient, False: uses single gradient \n",
    "optimizer_type = 'gd'           # options: 'gd': normal gradient descent, 'nesterov': nesterov acceleration\n",
    "num_iterations = 1000\n",
    "num_batches = 3\n",
    "avg_num=4\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=step_size,momentum= 0.9, use_nesterov = True)\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "losslist=[]\n",
    "rmean=[]\n",
    "\n",
    "defocus_grid=  1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, model.Nz * num_batches)) #mm or dioptres\n",
    "    \n",
    "\n",
    "if optimizer_type == 'nesterov':\n",
    "    tk = tf.constant(1,tf.float32)\n",
    "    tkp = tf.constant(1,tf.float32)\n",
    "\n",
    "    #xkp = model.variables\n",
    "    nvars = np.shape(model.variables)[0]\n",
    "    xk = []\n",
    "    xkp = []\n",
    "    [xk.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [xkp.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [tf.assign(xkp[n],model.variables[n]) for n in range(nvars)]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(num_iterations):\n",
    "    defocus_epoch = np.random.permutation(defocus_grid)\n",
    "    for j in range(0, num_batches):\n",
    "        for l in range(0,avg_num):\n",
    "            if use_averaged_gradient == True:\n",
    "                grad,lossvalue, Rmat =  averaged_gradient(model, loss, num_averages = 10)\n",
    "            else: \n",
    "                grad, lossvalue, Rmat = gradients_and_scaling(model, loss, defocus_epoch[j*model.Nz:j*model.Nz+model.Nz])  # initial value \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "        #grad,lossvalue, Rmat=gradient(model,loss)\n",
    "\n",
    "       # new_xpos = model.xpos - step_size*grad[2]\n",
    "       # new_ypos = model.xpos - step_size*grad[3] \n",
    "        #new_grad, test_dist, test_dist_bool = constrain_distances(model, new_xpos, new_ypos, grad) # apply constraint \n",
    "\n",
    "\n",
    "        # Gradient step\n",
    "            if optimizer_type == 'gd':\n",
    "                optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "                # Projection step\n",
    "                project_to_aper_keras(model)\n",
    "\n",
    "            if optimizer_type == 'nesterov':\n",
    "\n",
    "                optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\n",
    "                # Projection step\n",
    "                project_to_aper_keras(model)\n",
    "\n",
    "                # Update variables for next loop\n",
    "                tk = tkp\n",
    "                [tf.assign(xk[n],xkp[n]) for n in range(nvars)]\n",
    "\n",
    "                # Get state after project (gradient_step(yk))\n",
    "                [tf.assign(xkp[n],model.variables[n]) for n in range(nvars)]\n",
    "\n",
    "                #Acceleration\n",
    "                tkp = 0.5*(1.0 + tf.sqrt(1.0 + 4*tf.square(tk)))\n",
    "\n",
    "                bkp = (tk - 1)/tkp\n",
    "                ykp = [xkp[n] + bkp*(xkp[n] - xk[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "                # Update model variables (yk)\n",
    "                [model.variables[n].assign(ykp[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            T,aper=make_lenslet_tf(model)\n",
    "\n",
    "            losslist.append(lossvalue)\n",
    "\n",
    "            # Plotting Everything \n",
    "            plt.subplot(1,4,1)\n",
    "            plt.cla()\n",
    "            plt.imshow(T.numpy())\n",
    "            plt.title('surface')\n",
    "\n",
    "            plt.subplot(1,4,2)\n",
    "            plt.cla()\n",
    "            plt.semilogy(losslist)\n",
    "            plt.title('loss')\n",
    "\n",
    "            plt.subplot(1,4,3)\n",
    "            plt.cla()\n",
    "            pl = plt.plot(Rmat.numpy(),'k.')\n",
    "            plt.title('Rmat')\n",
    "\n",
    "            plt.subplot(1,4,4)\n",
    "            #plt.cla()\n",
    "            pl = plt.plot(model.rlist.numpy(),'k.')\n",
    "            plt.title('radii')\n",
    "\n",
    "\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            pl.clear()\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "#     pl.remove?\n",
    "    \n",
    "xposf = model.xpos.numpy()\n",
    "yposf = model.ypos.numpy()\n",
    "rlistf = model.rlist.numpy()\n",
    "offsetf = model.lenslet_offset.numpy() \n",
    "xpos0=xinit.numpy()\n",
    "ypos0=yinit.numpy()\n",
    "rlist0=rinit.numpy()\n",
    "offset0=offsetinit.numpy()\n",
    "save_dict = {'xpos0':xpos0 ,\n",
    "         'ypos0': ypos0,\n",
    "         'rlist0': rlist0,\n",
    "         'offset0': offset0,\n",
    "        'losses': losslist,\n",
    "         'xposf': xposf,\n",
    "         'yposf': yposf,\n",
    "         'rlistf': rlistf,\n",
    "         'offsetf': offsetf,}\n",
    "\n",
    "save_extension = 'test_'+ str(CA)+ '_' +'_initial_loss_' + str(losslist[0])+ '_final_loss_' + str(losslist[-1]) + '.mat'\n",
    "scipy.io.savemat('/home/kyrollos/randoscope/data_2_3_2019/' + save_extension, save_dict)\n",
    "\n",
    "\n",
    "#cbar = fig.colorbar(rshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io \n",
    "xposf = model.xpos.numpy()\n",
    "yposf = model.ypos.numpy()\n",
    "rlistf = model.rlist.numpy()\n",
    "offsetf = model.lenslet_offset.numpy() \n",
    "xpos0=xinit.numpy()\n",
    "ypos0=yinit.numpy()\n",
    "rlist0=rinit.numpy()\n",
    "offset0=offsetinit.numpy()\n",
    "save_dict = {'xpos0':xpos0 ,\n",
    "         'ypos0': ypos0,\n",
    "         'rlist0': rlist0,\n",
    "         'offset0': offset0,\n",
    "        'losses': losslist,\n",
    "         'xposf': xposf,\n",
    "         'yposf': yposf,\n",
    "         'rlistf': rlistf,\n",
    "         'offsetf': offsetf,}\n",
    "\n",
    "save_extension = 'test_'+ '21zplanes_noavg'+ '_' +'_initial_loss_' + str(losslist[0])+ '_final_loss_' + str(losslist[-1]) + '.mat'\n",
    "scipy.io.savemat('/home/kyrollos/randoscope/data_2_3_2019/' + save_extension, save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare spectra\n",
    "def radial_profile(data, center):\n",
    "    y, x = np.indices((data.shape))\n",
    "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    r = r.astype(np.int)\n",
    "\n",
    "    tbin = np.bincount(r.ravel(), data.ravel())\n",
    "    nr = np.bincount(r.ravel())\n",
    "    radialprofile = tbin / nr\n",
    "    return radialprofile\n",
    "\n",
    "def plot_spectra(model):\n",
    "    T,aper=make_lenslet_tf(model)\n",
    "    #model = Model()\n",
    "    psf_stack = model.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum = model.gen_stack_spectrum(psf_stack)\n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        ax[n].plot((model.dc_mask*tf.abs(stack_spectrum[n])).numpy()[0,:model.samples[1]],'r',label='spectrum z{}'.format(n))\n",
    "        ax[n].plot(model.target_F.numpy()[0,:model.samples[1]],'k',label='Target res: {}'.format(model.target_res))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Spectrum')\n",
    "        ax[n].set_ylim(top=5)\n",
    "        \n",
    "def compare_spectra(model_init, model_opt):\n",
    "    T,aper=make_lenslet_tf(model_init)\n",
    "    #model = Model()\n",
    "    psf_stack = model_init.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum_init = model_init.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    T,aper=make_lenslet_tf(model_opt)\n",
    "    #model = Model()\n",
    "    psf_stack = model_opt.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum_opt = model_opt.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        ax[n].plot((model_init.dc_mask*tf.abs(stack_spectrum_init[n])).numpy()[0,:model.samples[1]],'r',label='Initial spectrum z{}'.format(n))\n",
    "        ax[n].plot((model_opt.dc_mask*tf.abs(stack_spectrum_opt[n])).numpy()[0,:model.samples[1]],'k',label='Optimized spectrum z{}'.format(n))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Spectrum')\n",
    "        ax[n].set_ylim(top=5)\n",
    "        \n",
    "def plot_spectra_radial(model):\n",
    "    T,aper=make_lenslet_tf(model)\n",
    "    #model = Model()\n",
    "    psf_stack = model.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum = model.gen_stack_spectrum(psf_stack)\n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        radprof = radial_profile((model.dc_mask*tf.abs(stack_spectrum[n])).numpy(),(0,0))\n",
    "        ax[n].plot(radprof[:model.samples[1]],'r',label='spectrum z{}'.format(n))\n",
    "        ax[n].plot(model.target_F.numpy()[0,:model.samples[1]],'k',label='Target res: {}'.format(model.target_res))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Spectrum')\n",
    "        ax[n].set_ylim(top=5)\n",
    "\n",
    "        \n",
    "# radial average\n",
    "def compare_spectra_radial(model_init, model_opt):\n",
    "    T,aper=make_lenslet_tf(model_init)\n",
    "    #model = Model()\n",
    "    psf_stack = model_init.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum_init = model_init.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    T,aper=make_lenslet_tf(model_opt)\n",
    "    #model = Model()\n",
    "    psf_stack = model_opt.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum_opt = model_opt.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        \n",
    "        radprof_init = radial_profile((model_init.dc_mask*tf.abs(stack_spectrum_init[n])).numpy(),(0,0))\n",
    "        ax[n].plot(radprof_init[:model_init.samples[1]],'r',label='Init z{}'.format(n))\n",
    "        #ax[n].plot((model_init.dc_mask*tf.abs(stack_spectrum_init[n])).numpy()[0,:model.samples[1]],'r',label='Initial spectrum z{}'.format(n))\n",
    "        #ax[n].plot((model_opt.dc_mask*tf.abs(stack_spectrum_opt[n])).numpy()[0,:model.samples[1]],'k',label='Optimized spectrum z{}'.format(n))\n",
    "        radprof_opt = radial_profile((model_opt.dc_mask*tf.abs(stack_spectrum_opt[n])).numpy(),(0,0))\n",
    "        ax[n].plot(radprof_opt[:model_init.samples[1]],'k',label='Opt z{}'.format(n))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Radially averaged spectrum')\n",
    "        ax[n].set_ylim(top=5)\n",
    "    \n",
    "#model_opt = Model()\n",
    "#model_opt.load_weights('C:\\\\Users\\\\herbtipa\\\\logsumexp_1000_2p5.hd5')\n",
    "plot_spectra_radial(model_init)\n",
    "\n",
    "\n",
    "#re_init_model()\n",
    "plot_spectra_radial(model)\n",
    "compare_spectra_radial(model_init, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ropt = model(0).numpy()\n",
    "xcorr_opt = calc_max_xcorr(model)\n",
    "Rinit = model_init(0).numpy()\n",
    "xcorr_init = calc_max_xcorr(model_init)\n",
    "\n",
    "f, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(Ropt[:model.Nz], 'rx',label='optimized')\n",
    "ax[0].plot(Rinit[:model.Nz],  'k.',label='init')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Spectral fitting')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('z-plane')\n",
    "\n",
    "\n",
    "ax[1].plot(xcorr_opt, 'rx',label='optimized')\n",
    "ax[1].plot(xcorr_init,  'k.',label='init')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Cross correlations')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('z-plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_psf_stack(model):\n",
    "    T,aper=make_lenslet_tf(model)\n",
    "    test2 = model.gen_psf_stack(T, aper, .5)\n",
    "    test = model.gen_stack_spectrum(test2)\n",
    "    test3=model.gen_correlation_stack(test)\n",
    "    #ax[0].imshow(T)\n",
    "    #plt.plot((model.dc_mask * tf.abs(test[1]))[-1,:200].numpy())\n",
    "    #plt.plot((model.dc_mask * model.target_F)[-1,:200].numpy())\n",
    "    for z in range(len(test2)):\n",
    "        fig = plt.figure(figsize=(15,5))\n",
    "        plt.cla()\n",
    "        plt.imshow(test2[z].numpy(),vmax=0.005)\n",
    "        #plt.plot(np.abs(test3[z][model.samples[0],:]))\n",
    "\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.pause(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_psf_stack(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, .5)\n",
    "plt.imshow(test2[0],vmin=0,vmax=0.01)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ior-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting diameter vs z!\n",
    "rlist = 1/(np.linspace(1/model.Rmax, 1/model.Rmin, model.Nlenslets))\n",
    "flist=rlist/0.56\n",
    "Ag=0.0725\n",
    "Cg=-0.5990\n",
    "Mag=Ag*(1-model.t/flist)+model.t*Cg\n",
    "diameter=1.22*model.lam*model.t/(Mag*model.target_res)\n",
    "diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def psf_slider(z, psf_stack):\n",
    "    \n",
    "    #test = model.gen_stack_spectrum(test2)\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    plt.imshow(psf_stack[z].numpy())\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_list = 1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, 15)) #mm or dioptres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, .5, zplanes_opt = z_list)\n",
    "np.shape(test2[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "interact(psf_slider, z = widgets.IntSlider(min=1, max = 15, step=1, value=1), test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_psf_stack(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to run reconstructions in loop (to run many initializations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_opt(iterations, model, ind, CA):\n",
    "    \n",
    "    xpos0 = model.xpos.numpy()\n",
    "    ypos0 = model.ypos.numpy()\n",
    "    rlist0 = model.rlist.numpy()\n",
    "    offset0 = model.lenslet_offset.numpy()\n",
    "    \n",
    "    step_size = 1e-8\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "\n",
    "    losslist=[]\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        num_grads = 10\n",
    "        grad_averaged = 1/num_grads * gradients_and_scaling(model, loss)\n",
    "        for g in range(0,num_grads-1):\n",
    "            grads, lossvalue = gradients_and_scaling(model, loss)\n",
    "        \n",
    "            for grad_ind in range(0,len(model.variables)):\n",
    "                grad_averaged[grad_ind] = grad_averaged[grad_ind] + 1/num_grads * grads[grad_ind]\n",
    "\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads_averaged,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "        project_to_aper_keras(model)\n",
    "        T,aper=make_lenslet_tf(model)\n",
    "\n",
    "        losslist.append(lossvalue.numpy())\n",
    "        \n",
    "    xposf = model.xpos.numpy()\n",
    "    yposf = model.ypos.numpy()\n",
    "    rlistf = model.rlist.numpy()\n",
    "    offsetf = model.lenslet_offset.numpy()    \n",
    "    save_dict = {'xpos0': xpos0,\n",
    "             'ypos0': ypos0,\n",
    "             'rlist0': rlist0,\n",
    "             'offset0': offset0,\n",
    "            'losses': losslist,\n",
    "             'xposf': xposf,\n",
    "             'yposf': yposf,\n",
    "             'rlistf': rlistf,\n",
    "             'offsetf': offsetf,}\n",
    "    \n",
    "    save_extension = 'test_'+ str(CA)+ '_' + str(ind)+'_initial_loss_' + str(losslist[0])+ '_final_loss_' + str(losslist[-1]) + '.mat'\n",
    "    scipy.io.savemat('/home/kyrollos/randoscope/data_1_12_2019/' + save_extension, save_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Re-initialize model\n",
    "model.xpos.assign(xinit)\n",
    "model.ypos.assign(yinit)\n",
    "model.lenslet_offset.assign(offsetinit)\n",
    "model.rlist.assign(rinit)\n",
    "print(model.xpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#[model.variables[n].assign(init_vars) for n in range(nvars)]\n",
    "# Gradient descent\n",
    "step_size = 1e-9\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "losslist=[]\n",
    "rmean=[]\n",
    "tk = tf.constant(1,tf.float32)\n",
    "tkp = tf.constant(1,tf.float32)\n",
    "# xk = model.variables\n",
    "# xkp = model.variables\n",
    "# nvars = np.shape(xk)[0]\n",
    "for i in range(70):\n",
    "    grad,lossvalue, Rmat=gradient(model,loss_sum)\n",
    "\n",
    "   # new_xpos = model.xpos - step_size*grad[2]\n",
    "   # new_ypos = model.xpos - step_size*grad[3] \n",
    "\n",
    "    #new_grad, test_dist, test_dist_bool = constrain_distances(model, new_xpos, new_ypos, grad) # apply constraint \n",
    "    #print(new_grad)\n",
    "    grad[1] = grad[1] * 100000  #Radius\n",
    "    grad[2]=grad[2]*10000 # X\n",
    "    grad[3]=grad[3]*10000 # Y\n",
    "    #grad[2] = grad[2]*new_grad[0,:]*10000\n",
    "   # grad[3] = grad[3]*new_grad[1,:]*10000   # update the gradient\n",
    "#    grad[1] = grad[1]*\n",
    "    \n",
    "    #grads=remove_nan_gradients(grad)\n",
    "    #print(grads)\n",
    "    \n",
    "    # Gradient step\n",
    "    optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    # Projection step\n",
    "    project_to_aper_keras(model)\n",
    "    \n",
    "\n",
    "    \n",
    "    T,aper=make_lenslet_tf(model)\n",
    "\n",
    "    losslist.append(lossvalue)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.cla()\n",
    "    #plt.plot(model.xpos.numpy(),model.ypos.numpy(),'o')\n",
    "    #plt.axis('equal')\n",
    "    #rmean.append(tf.reduce_mean(model.rlist.numpy()))\n",
    "    #plt.plot(rmean)\n",
    "    plt.imshow(T.numpy())\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.cla()\n",
    "    plt.semilogy(losslist)\n",
    "    \n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.cla()\n",
    "\n",
    "    #rshow = plt.imshow(np.tril(Rmat[model.Nz+1:-1])\n",
    "#     cbar = fig.colorbar(rshow)    \n",
    "    pl = plt.plot(Rmat.numpy(),'k.')\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    pl.clear()\n",
    "#     cbar.remove()\n",
    "# cbar = fig.colorbar(rshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,3))\n",
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, .5)\n",
    "test = model.gen_stack_spectrum(test2)\n",
    "test4=model.gen_correlation_stack(test)\n",
    "#ax[0].imshow(T)\n",
    "\n",
    "\n",
    "# for z in range(len(test4)):\n",
    "\n",
    "#     #ax[1].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z])))), vmin = 0, vmax = 50)\n",
    "#     #ax[2].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z]))) - model.target_F), vmin = -50, vmax = 50)\n",
    "#     plt.cla()\n",
    "#     #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))), vmax = 50)\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.plot(np.abs(test3[z][model.samples[0],:]))\n",
    "    \n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.plot(np.abs(test4[z][model.samples[0],:]))\n",
    "\n",
    "#     display.display(fig)\n",
    "#     display.clear_output(wait=True)\n",
    "#     plt.pause(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "for z in range(7):\n",
    "\n",
    "    #ax[1].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z])))), vmin = 0, vmax = 50)\n",
    "    #ax[2].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z]))) - model.target_F), vmin = -50, vmax = 50)\n",
    "    #plt.cla()\n",
    "    #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))), vmax = 50)\n",
    "    plt.subplot(7,2,z*2+1)\n",
    "    ind = (model.Nz+1) * (z)\n",
    "    plt.plot(np.abs(test3[ind][model.samples[0],:]))\n",
    "    \n",
    "    plt.subplot(7,2,z*2+2)\n",
    "    plt.plot(np.abs(test4[ind][model.samples[0],:]))\n",
    "\n",
    "    #display.display(fig)\n",
    "    #display.clear_output(wait=True)\n",
    "    #plt.pause(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(tf.reshape(model(0),(model.Nz, model.Nz)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.sort(1/model.rlist.numpy()),'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.lenslet_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, 0.5)\n",
    "test = model.gen_stack_spectrum(test2)\n",
    "fig = plt.figure()\n",
    "\n",
    "f,ax = plt.subplots(1,2,figsize=(20,20))\n",
    "\n",
    "ax[0].imshow(T)\n",
    "\n",
    "\n",
    "for z in range(model.Nz):\n",
    "    #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))) - model.target_F, vmax = 50)\n",
    "    im_disp = ax[1].imshow((tf.real((test2[z]))),vmax = .3)\n",
    "    cbar = plt.colorbar(im_disp, ax=ax[1])\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    cbar.remove()\n",
    "    #plt.pause(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def animate_zstack(i):\n",
    "#    ax[1].clear()\n",
    "    ax[1].imshow(tf.real(test2[i]),vmax = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.save_weights('C:\\\\Users\\\\herbtipa\\\\lenslets_one_per_depth.hd5',overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(20,20))\n",
    "ax[0].imshow(T)\n",
    "ani = animation.FuncAnimation(f, animate_zstack, frames = range(model.Nz), interval = 500, repeat = False)\n",
    "# plt.show(ani)\n",
    "#Writer = animation.writers['ffmpeg']\n",
    "\n",
    "# HTML(ani.to_html5_video())\n",
    "for i in range(model.Nz):\n",
    "    animate_zstack(i)\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    f.savefig('C:\\\\Users\\\\herbtipa\\\\foo_{}.png'.format(i))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#jacobian=[]\n",
    "#for i in range (model.Nz**2):\n",
    "#    with tf.GradientTape() as tape:\n",
    "#        #tape.watch(model.variables)\n",
    "#        R=model(0)\n",
    "#        jacobian.append(tape.gradient(R[i], model.variables))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
