{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "#%load_ext autoreload   \n",
    "#%autoreload 2\n",
    "\n",
    "#Autoreloading of modules so you don't have to restart kernel after editing .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tf.enable_eager_execution()\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from miniscope_utils_tfn import *\n",
    "from miniscope_modeln import Model as msu_model\n",
    "#import utils as krist\n",
    "import scipy.misc as sc\n",
    "import scipy.ndimage as ndim\n",
    "import scipy.misc as misc\n",
    "from scipy import signal\n",
    "import scipy.io\n",
    "from skimage.transform import resize as imresize\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import time\n",
    "from itertools import permutations\n",
    "from itertools import combinations\n",
    "#import copy\n",
    "#from bridson import poisson_disc_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#device = '/gpu:0'\n",
    "#print(tf.test.is_gpu_available())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def distances(model, x,y):\n",
    "    dist = []\n",
    "    dist_bool = []\n",
    "    things = np.arange(model.Nlenslets)\n",
    "    test_perm = list(permutations(things, 2))\n",
    "\n",
    "    for i in range(0, len(test_perm)):\n",
    "        dist_i = tf.sqrt(tf.square(x[test_perm[i][0]]-x[test_perm[i][1]])+tf.square(y[test_perm[i][0]]-y[test_perm[i][1]]))\n",
    "        dist.append(dist_i)\n",
    "        dist_bool.append(dist_i>0*model.mean_lenslet_CA)  ##fix later\n",
    "        \n",
    "    return dist, dist_bool, test_perm\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick Zernike Polynomials\n",
    "[ 0: piston, 1: tilt, 2: tilt, 3: astigmatism, 4: defocus, 5: astigmatism, 6: trefoil, 7: coma, 8: coma, 9: trefoil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zernikes_index = [0, 1, 2, 3, 5]  \n",
    "#model = msu_model(target_res=0.004,lenslet_CA=0.2,aberrations = True)  # zsampling options: 'fixed' or 'uniform_random'\n",
    "#model_init=msu_model(target_res=0.004,lenslet_CA=0.2,aberrations = True)\n",
    "model = msu_model(target_res=0.004, lenslet_CA=0.2,aberrations = False) # zsampling options: 'fixed' or 'uniform_random'\n",
    "#model = msu_model(target_res=0.004, aberrations = True, zernikes = zernikes_index)  # zsampling options: 'fixed' or 'uniform_random'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: find the best initialization before running the model.  \n",
    "Warning: this takes a long time, so only run this if you want to redo this step, otherwise load the best initilization from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_init = False\n",
    "# if find_best_init  == True:\n",
    "#     print('Finding best initialization')\n",
    "#     find_best_initialization(model, num_trials = 2000, save_results = False) \n",
    "\n",
    "load_init_from_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(model, filename):\n",
    "    loaded = scipy.io.loadmat(filename)\n",
    "    tf.assign(model.xpos, tfe.Variable(loaded['x_best'].ravel()))\n",
    "    tf.assign(model.ypos, tfe.Variable(loaded['y_best'].ravel()))\n",
    "    tf.assign(model.rlist, tfe.Variable(loaded['r_best'].ravel()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model = msu_model()  # zsampling options: 'fixed' or 'uniform_random'\n",
    "#plt.figure(figsize=(20,20))\n",
    "#plt.plot(model.xg[model.samples[1]//2:model.samples[1]//2 + 50]*1000,model.target_airy.numpy()[model.samples[0]//2,model.samples[1]//2:model.samples[1]//2 + 50])\n",
    "#plt.imshow(model.target_F.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (15,10))\n",
    "#model = load_model_from_file(model, file_best)\n",
    "#T1 ,_,T2 = make_lenslet_tf_zern(model)\n",
    "#R_model = model(0)\n",
    "#plt.subplot(2,2,1); plt.imshow(T1); plt.title('best'); \n",
    "#plt.subplot(2,2,2); plt.plot(R_model.numpy(), 'x'); plt.title('R best')\n",
    "\n",
    "#model = load_model_from_file(model, file_worst)\n",
    "#T1 ,_,T2 = make_lenslet_tf_zern(model)\n",
    "#R_model = model(0)\n",
    "#plt.subplot(2,2,3); plt.imshow(T1); plt.title('worst')\n",
    "#plt.subplot(2,2,4); plt.plot(R_model.numpy(), 'x'); plt.title('R worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model = msu_model(target_res=0.004,aberrations = True)  # zsampling options: 'fixed' or 'uniform_random'\n",
    "\n",
    "# Load initialization from file\n",
    "if load_init_from_file == True:\n",
    "    print('loading initilization from file')\n",
    "    file_best = '/home/kyrollos/randoscope/BestAndWorst/best_init37nanoscribeZemaxCorrected40CorrectFocals.mat'\n",
    "    file_worst = '/home/kyrollos/randoscope/BestAndWorst/worst_init37nanoscribeZemaxCorrected40CorrectFocals.mat'\n",
    "    model = load_model_from_file(model, file_best)\n",
    "    \n",
    "# Save initial values for later comparison \n",
    "Rmat=model(0)\n",
    "R_init = Rmat\n",
    "Tinit,_,_=make_lenslet_tf_zern(model)\n",
    "\n",
    "xinit = tfe.Variable(tf.zeros(model.Nlenslets));       tf.assign(xinit, model.xpos)\n",
    "yinit = tfe.Variable(tf.zeros(model.Nlenslets));       tf.assign(yinit, model.ypos)\n",
    "rinit = tfe.Variable(tf.zeros(model.Nlenslets));       tf.assign(rinit, model.rlist)\n",
    "#offsetinit = tfe.Variable(tf.zeros(model.Nlenslets));  tf.assign(offsetinit, model.lenslet_offset)\n",
    "#zerninit = tfe.Variable(tf.zeros((model.Nlenslets, model.numzern))); tf.assign(zerninit, model.zernlist)\n",
    "\n",
    "model_init = msu_model(target_res=0.004, lenslet_CA=0.2,aberrations = False)#, zernikes = zernikes_index)\n",
    "tf.assign(model_init.xpos, xinit)\n",
    "tf.assign(model_init.ypos, yinit)\n",
    "tf.assign(model_init.rlist, rinit)\n",
    "#tf.assign(model_init.lenslet_offset, offsetinit)\n",
    "#tf.assign(model_init.zernlist, zerninit)\n",
    "Rmat_init = model_init(0)\n",
    "\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Tinit.numpy())\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(R_init.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zernikes = False\n",
    "if test_zernikes == True:\n",
    "    zernikes_index = [5, 6]\n",
    "    model.zernikes = zernikes_index;\n",
    "    zerninit = tfe.Variable(tf.ones((model.Nlenslets, model.numzern))*.00001); \n",
    "    tf.assign(model.zernlist, zerninit);\n",
    "\n",
    "    T1 ,_,T2 = make_lenslet_tf_zern(model)\n",
    "    R_model = model(0)\n",
    "    plt.figure(figsize = (20,5))\n",
    "    plt.subplot(1,4,1), plt.imshow(T2); plt.colorbar(); plt.title('no aberrations')\n",
    "    plt.subplot(1,4,2), plt.imshow(T1); plt.colorbar(); plt.title('with aberrations')\n",
    "    plt.subplot(1,4,3), plt.imshow(np.abs(T1-T2)); plt.colorbar(); plt.title('difference')\n",
    "    #plt.subplot(1,4,4), plt.plot(R_model.numpy(), 'x'); plt.title('Rlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_noaber = msu_model(target_res=0.004,aberrations = False)\n",
    "tf.assign(model_noaber.xpos, xinit)\n",
    "tf.assign(model_noaber.ypos, yinit)\n",
    "tf.assign(model_noaber.rlist, rinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dc_mask = np.ones_like(model.target_F.numpy())  #Mask for DC. Set to zero anywhere we want to ignore loss computation of power spectrum\n",
    "# dc_mask[:2,:2] = 0\n",
    "# dc_mask[-1,0] = 0\n",
    "# dc_mask[0,-1] = 0\n",
    "# dc_mask[-1,-1] = 0\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(dc_mask)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.load_weights(\"C:\\\\Users\\\\herbtipa\\\\lenslets_one_per_depth\")\n",
    "#print(model.dc_mask[-1,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotting_xcorr = False\n",
    "if plotting_xcorr == True:\n",
    "    T,aper,_=make_lenslet_tf_zern(model)\n",
    "    test2 = model.gen_psf_stack(T, aper, .5)\n",
    "    test = model.gen_stack_spectrum(test2)\n",
    "    test3=model.gen_correlation_stack(test)\n",
    "    #ax[0].imshow(T)\n",
    "    #plt.plot((model.dc_mask * tf.abs(test[1]))[-1,:200].numpy())\n",
    "    #plt.plot((model.dc_mask * model.target_F)[-1,:200].numpy())\n",
    "    for z in range(10):\n",
    "        fig = plt.figure(figsize=(15,5))\n",
    "        plt.cla()\n",
    "        plt.plot(np.abs(test3[z][model.samples[0],:]))\n",
    "\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.pause(.5)\n",
    "\n",
    "    #     #ax[1].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z])))), vmin = 0, vmax = 50)\n",
    "    #     #ax[2].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z]))) - model.target_F), vmin = -50, vmax = 50)\n",
    "    #     plt.cla()\n",
    "    #     #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))), vmax = 50)\n",
    "    #     #plt.imshow(tf_fftshift(np.abs(test[z])))\n",
    "    #     plt.plot(tf_fftshift(tf.abs(test[z]))[768,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to return  maximum cross correlation\n",
    "def calc_max_xcorr(model):\n",
    "    \n",
    "    T,aper, _=make_lenslet_tf_zern(model)\n",
    "    #model = Model()\n",
    "    psf_stack = model.gen_psf_stack(T, aper, 0.5)\n",
    "    stack_spectrum = model.gen_stack_spectrum(psf_stack)\n",
    "    ccor_mx = []\n",
    "    for zref in range(model.Nz):\n",
    "        for z in range(model.Nz):\n",
    "            xcorr12 = tf.real(tf_fftshift(tf.ifft2d(stack_spectrum[zref]* tf.conj(stack_spectrum[z]))))\n",
    "            if z != zref:\n",
    "                ccor_mx.append(tf.reduce_max(xcorr12)) \n",
    "                    \n",
    "    return ccor_mx\n",
    "\n",
    "def re_init_model(model_in):\n",
    "    model_in.xpos.assign(xinit)\n",
    "    model_in.ypos.assign(yinit)\n",
    "\n",
    "    #model_in.lenslet_offset.assign(offsetinit)\n",
    "    model_in.rlist.assign(rinit)\n",
    "    model_in.zernlist.assign(zerninit)\n",
    "\n",
    "# Moved losses to utilities file\n",
    "#have tf do everything for us\n",
    "#def loss (model, inputs):\n",
    "#    Rmat = model(inputs)\n",
    "#    return tf.reduce_sum(tf.square(Rmat)), Rmat\n",
    "\n",
    "#def loss_sum(model, inputs):\n",
    "#    Rmat = model(inputs)\n",
    "#    return tf.reduce_sum(Rmat), Rmat\n",
    "\n",
    "#def loss_inf(model, inputs):\n",
    "#    Rmat = model(inputs)\n",
    "#    return tf.reduce_max(Rmat), Rmat\n",
    "\n",
    "#def loss_mixed(model, inputs):\n",
    "#   # max of off diags, sum of diags\n",
    "#    Rmat = model(inputs)\n",
    "#    diag_vec = Rmat[0:model.Nz]\n",
    "#    off_diag = Rmat[model.Nz+1:-1]\n",
    "#    return tf.reduce_sum(tf.abs(diag_vec)) + tf.reduce_max(off_diag), Rmat\n",
    "\n",
    "def gradient (model, myloss, inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        lossvalue, Rmat = myloss(model, inputs)\n",
    "        return tape.gradient(lossvalue, model.variables),lossvalue, Rmat\n",
    "    \n",
    "def gradients_and_scaling(model, loss, inputs):\n",
    "    grad,lossvalue, Rmat=gradient(model,loss, inputs)\n",
    "    \n",
    "    #grad[0] = grad[0] * 1000\n",
    "    #grad[1] = grad[1] * 1000\n",
    "    #grad[2] = grad[2] * 1000\n",
    "    \n",
    "    #grad[1] = grad[1] * 50000\n",
    "    #grad[2] = grad[2] * 5000\n",
    "    #grad[3] = grad[3] * 5000\n",
    "\n",
    "    grads=remove_nan_gradients(grad)\n",
    "    return grads, lossvalue, Rmat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def averaged_gradient(model, loss, num_averages = 10):\n",
    "\n",
    "    grad_averaged, lossvalue, Rmat = gradients_and_scaling(model, loss)  # initial value \n",
    "    batch_loss = lossvalue\n",
    "    Rmat_avg = Rmat\n",
    "    \n",
    "    for g in range(0, num_averages-1):\n",
    "        grads, lossvalue, Rmat = gradients_and_scaling(model, loss)\n",
    "        batch_loss = batch_loss + lossvalue\n",
    "        Rmat_avg = Rmat_avg + Rmat\n",
    "        \n",
    "        \n",
    "        for grad_ind in range(0,len(model.variables)):\n",
    "            grad_averaged[grad_ind] = grad_averaged[grad_ind] + grads[grad_ind]\n",
    "\n",
    "\n",
    "    for grad_ind2 in range(0,len(model.variables)):\n",
    "            grad_averaged[grad_ind2] = grad_averaged[grad_ind2]*1/num_averages\n",
    "\n",
    "    batch_loss = batch_loss / num_averages\n",
    "    Rmat_avg = Rmat_avg/num_averages\n",
    "    \n",
    "    return grad_averaged, batch_loss, Rmat_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def constrain_distances(model, new_xpos, new_ypos, grads):\n",
    "    test_dist, test_dist_bool, perm = distances(model, new_xpos, new_ypos)\n",
    "    grads = np.ones((2,model.Nlenslets))\n",
    "    for i in range(0,len(perm)):\n",
    "        if test_dist_bool[i].numpy() == False:\n",
    "            index = perm[i][0]\n",
    "            grads[0,index] = 0\n",
    "            grads[1,index] = 0\n",
    "    return grads, test_dist, test_dist_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_init_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Options:\n",
    "step_size = 1e-3   #1e-8 works well for l2\n",
    "use_averaged_gradient = False  # True: uses averaged gradient, False: uses single gradient \n",
    "optimizer_type = 'gd'           # options: 'gd': normal gradient descent, 'nesterov': nesterov acceleration\n",
    "num_iterations = 500\n",
    "num_batches = 1\n",
    "randomize_z = False   #If true, randomize zlist each epoch. If false, leave original order.\n",
    "\n",
    "#optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=step_size)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=step_size,momentum= 0.9, use_nesterov = True)\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "losslist=[]\n",
    "rmean=[]\n",
    "\n",
    "defocus_grid=  1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, model.Nz * num_batches)) #mm or dioptres\n",
    "    \n",
    "\n",
    "if optimizer_type == 'nesterov':\n",
    "    tk = tf.constant(1,tf.float32)\n",
    "    tkp = tf.constant(1,tf.float32)\n",
    "\n",
    "    #xkp = model.variables\n",
    "    nvars = np.shape(model.variables)[0]\n",
    "    xk = []\n",
    "    xkp = []\n",
    "    [xk.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [xkp.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [tf.assign(xkp[n],model.variables[n]) for n in range(nvars)]\n",
    "\n",
    "    \n",
    "for i in range(num_iterations):\n",
    "    if randomize_z:\n",
    "        defocus_epoch = np.random.permutation(defocus_grid)\n",
    "    else:\n",
    "        defocus_epoch = defocus_grid\n",
    "    for j in range(0, num_batches):\n",
    "        if use_averaged_gradient == True:\n",
    "            grad,lossvalue, Rmat =  averaged_gradient(model, loss, num_averages = 10)\n",
    "        else: \n",
    "            grad, lossvalue, Rmat = gradients_and_scaling(model, loss, defocus_epoch[j*model.Nz:j*model.Nz+model.Nz])  # initial value \n",
    "\n",
    "\n",
    "        #grad,lossvalue, Rmat=gradient(model,loss)\n",
    "\n",
    "       # new_xpos = model.xpos - step_size*grad[2]\n",
    "       # new_ypos = model.xpos - step_size*grad[3] \n",
    "        #new_grad, test_dist, test_dist_bool = constrain_distances(model, new_xpos, new_ypos, grad) # apply constraint \n",
    "\n",
    "\n",
    "        # Gradient step\n",
    "        if optimizer_type == 'gd':\n",
    "            optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "            # Projection step\n",
    "            project_to_aper_keras(model)\n",
    "\n",
    "        if optimizer_type == 'nesterov':\n",
    "\n",
    "            optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\n",
    "            # Projection step\n",
    "            project_to_aper_keras(model)\n",
    "\n",
    "            # Update variables for next loop\n",
    "            tk = tkp\n",
    "            [tf.assign(xk[n],xkp[n]) for n in range(nvars)]\n",
    "\n",
    "            # Get state after project (gradient_step(yk))\n",
    "            [tf.assign(xkp[n],model.variables[n]) for n in range(nvars)]\n",
    "\n",
    "            #Acceleration\n",
    "            tkp = 0.5*(1.0 + tf.sqrt(1.0 + 4*tf.square(tk)))\n",
    "\n",
    "            bkp = (tk - 1)/tkp\n",
    "            ykp = [xkp[n] + bkp*(xkp[n] - xk[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "            # Update model variables (yk)\n",
    "            [model.variables[n].assign(ykp[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        T,aper,T2=make_lenslet_tf_zern(model)\n",
    "\n",
    "        losslist.append(lossvalue)\n",
    "\n",
    "        # Plotting Everything \n",
    "        plt.subplot(2,4,1)\n",
    "        plt.cla()\n",
    "        plt.imshow(T.numpy())\n",
    "        plt.title('surface')\n",
    "\n",
    "        plt.subplot(2,4,2)\n",
    "        plt.cla()\n",
    "        plt.semilogy(losslist)\n",
    "        plt.title('loss')\n",
    "\n",
    "        plt.subplot(2,4,3)\n",
    "        plt.cla()\n",
    "        pl = plt.plot(Rmat.numpy(),'k.')\n",
    "        plt.title('Rmat')\n",
    "\n",
    "        plt.subplot(2,4,4)\n",
    "        plt.cla()\n",
    "        plt.plot(model.lenslet_offset.numpy()); #plt.colorbar()\n",
    "        plt.title('Offset')\n",
    "        \n",
    "        plt.subplot(2,4,5)\n",
    "        #plt.cla()\n",
    "        plt.plot(model.rlist.numpy(),'k.'); #plt.colorbar()\n",
    "        plt.title('rlist')\n",
    "        \n",
    "        plt.subplot(2,4,6)\n",
    "        #plt.cla()\n",
    "        plt.plot(model.xpos.numpy(),'k.'); #plt.colorbar()\n",
    "        plt.title('xpos')\n",
    "        \n",
    "        plt.subplot(2,4,7)\n",
    "        #plt.cla()\n",
    "        plt.plot(model.ypos.numpy(),'k.'); #plt.colorbar()\n",
    "        plt.title('ypos')\n",
    "        #pl = plt.plot(model.rlist.numpy(),'k.')\n",
    "        #plt.title('radii')\n",
    "        \n",
    "#         plt.subplot(1,5,5)\n",
    "#         plt.cla()\n",
    "#         plt.plot(grad[-1][:,0].numpy())\n",
    "#        # plt.plot(grad[-1][:,1].numpy())\n",
    "#         #plt.plot(grad[-1][:,2].numpy())\n",
    "#        # plt.plot(grad[-1][:,3].numpy())\n",
    "#        # plt.plot(grad[-1][:,4].numpy())\n",
    "#         plt.legend(('0', '1', '2', '3', '4'))\n",
    "\n",
    "\n",
    "#         display.display(plt.gcf())\n",
    "#         display.clear_output(wait=True)\n",
    "        pl.clear()\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "#     pl.remove?\n",
    "    \n",
    "\n",
    "\n",
    "#cbar = fig.colorbar(rshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options:\n",
    "step_size = 1e-6   #1e-8 works well for l2\n",
    "use_averaged_gradient = False  # True: uses averaged gradient, False: uses single gradient \n",
    "optimizer_type = 'gd'           # options: 'gd': normal gradient descent, 'nesterov': nesterov acceleration\n",
    "num_iterations = 200\n",
    "num_batches = 1\n",
    "randomize_z = False   #If true, randomize zlist each epoch. If false, leave original order.\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=step_size,momentum= 0.9, use_nesterov = True)\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "losslist=[]\n",
    "rmean=[]\n",
    "\n",
    "defocus_grid=  1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, model.Nz * num_batches)) #mm or dioptres\n",
    "    \n",
    "\n",
    "if optimizer_type == 'nesterov':\n",
    "    tk = tf.constant(1,tf.float32)\n",
    "    tkp = tf.constant(1,tf.float32)\n",
    "\n",
    "    #xkp = model.variables\n",
    "    nvars = np.shape(model.variables)[0]\n",
    "    xk = []\n",
    "    xkp = []\n",
    "    [xk.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [xkp.append(tf.Variable(tf.zeros(model.Nlenslets))) for n in range(nvars)]\n",
    "    [tf.assign(xkp[n],model.variables[n]) for n in range(nvars)]\n",
    "\n",
    "    \n",
    "for i in range(num_iterations):\n",
    "    if randomize_z:\n",
    "        defocus_epoch = np.random.permutation(defocus_grid)\n",
    "    else:\n",
    "        defocus_epoch = defocus_grid\n",
    "    for j in range(0, num_batches):\n",
    "        if use_averaged_gradient == True:\n",
    "            grad,lossvalue, Rmat =  averaged_gradient(model_noaber, loss, num_averages = 10)\n",
    "        else: \n",
    "            grad, lossvalue, Rmat = gradients_and_scaling(model_noaber, loss, defocus_epoch[j*model.Nz:j*model.Nz+model.Nz])  # initial value \n",
    "\n",
    "\n",
    "        #grad,lossvalue, Rmat=gradient(model,loss)\n",
    "\n",
    "       # new_xpos = model.xpos - step_size*grad[2]\n",
    "       # new_ypos = model.xpos - step_size*grad[3] \n",
    "        #new_grad, test_dist, test_dist_bool = constrain_distances(model, new_xpos, new_ypos, grad) # apply constraint \n",
    "\n",
    "\n",
    "        # Gradient step\n",
    "        if optimizer_type == 'gd':\n",
    "            optimizer.apply_gradients(zip(grad,model_noaber.variables),global_step=tf.train.get_or_create_global_step())\n",
    "            # Projection step\n",
    "            project_to_aper_keras(model_noaber)\n",
    "\n",
    "        if optimizer_type == 'nesterov':\n",
    "\n",
    "            optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\n",
    "            # Projection step\n",
    "            project_to_aper_keras(model)\n",
    "\n",
    "            # Update variables for next loop\n",
    "            tk = tkp\n",
    "            [tf.assign(xk[n],xkp[n]) for n in range(nvars)]\n",
    "\n",
    "            # Get state after project (gradient_step(yk))\n",
    "            [tf.assign(xkp[n],model_noaber.variables[n]) for n in range(nvars)]\n",
    "\n",
    "            #Acceleration\n",
    "            tkp = 0.5*(1.0 + tf.sqrt(1.0 + 4*tf.square(tk)))\n",
    "\n",
    "            bkp = (tk - 1)/tkp\n",
    "            ykp = [xkp[n] + bkp*(xkp[n] - xk[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "            # Update model variables (yk)\n",
    "            [model_noaber.variables[n].assign(ykp[n]) for n in range(nvars)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        T,aper,T2=make_lenslet_tf_zern(model_noaber)\n",
    "\n",
    "        losslist.append(lossvalue)\n",
    "\n",
    "        # Plotting Everything \n",
    "        plt.subplot(1,4,1)\n",
    "        plt.cla()\n",
    "        plt.imshow(T.numpy())\n",
    "        plt.title('surface')\n",
    "\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.cla()\n",
    "        plt.semilogy(losslist)\n",
    "        plt.title('loss')\n",
    "\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.cla()\n",
    "        pl = plt.plot(Rmat.numpy(),'k.')\n",
    "        plt.title('Rmat')\n",
    "\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.cla()\n",
    "        #plt.imshow(np.abs(T-T2)); #plt.colorbar()\n",
    "        \n",
    "        #pl = plt.plot(model.rlist.numpy(),'k.')\n",
    "        #plt.title('radii')\n",
    "\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        pl.clear()\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "#     pl.remove?\n",
    "    \n",
    "\n",
    "\n",
    "#cbar = fig.colorbar(rshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tnone,_,_= make_lenslet_tf_zern(model_noaber)\n",
    "Tsome,_,_ =make_lenslet_tf_zern(model)\n",
    "\n",
    "plt.imshow(Tnone-Tsome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rnone = model_noaber(0)\n",
    "Rsome = model(0)\n",
    "\n",
    "plt.plot(Rnone.numpy()), 'x'; plt.plot(Rsome.numpy(), 'o'); plt.legend(('no aberrations', 'aberrations'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1,aper,T2=make_lenslet_tf_zern(model)\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.subplot(1,3,1), plt.imshow(T2); plt.colorbar(); plt.title('no aberrations')\n",
    "plt.subplot(1,3,2), plt.imshow(T1); plt.colorbar(); plt.title('with aberrations')\n",
    "plt.subplot(1,3,3), plt.imshow(np.abs(T1-T2)); plt.colorbar(); plt.title('difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare spectra\n",
    "def radial_profile(data, center):\n",
    "    y, x = np.indices((data.shape))\n",
    "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    r = r.astype(np.int)\n",
    "\n",
    "    tbin = np.bincount(r.ravel(), data.ravel())\n",
    "    nr = np.bincount(r.ravel())\n",
    "    radialprofile = tbin / nr\n",
    "    return radialprofile\n",
    "\n",
    "def plot_spectra(model):\n",
    "    T,aper=make_lenslet_tf(model)\n",
    "    #model = Model()\n",
    "    psf_stack = model.gen_psf_stack(T, aper, 0.)\n",
    "    stack_spectrum = model.gen_stack_spectrum(psf_stack)\n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        ax[n].plot((model.dc_mask*tf.abs(stack_spectrum[n])).numpy()[0,:model.samples[1]//2],'r',label='spectrum z{}'.format(n))\n",
    "        ax[n].plot(model.target_F.numpy()[0,:model.samples[1]//2],'k',label='Target res: {}'.format(model.target_res))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Spectrum')\n",
    "        ax[n].set_ylim(top=5)\n",
    "        \n",
    "def compare_spectra(model_init, model_opt):\n",
    "    T,aper=make_lenslet_tf(model_init)\n",
    "    #model = Model()\n",
    "    psf_stack = model_init.gen_psf_stack(T, aper, 0.)\n",
    "    stack_spectrum_init = model_init.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    T,aper=make_lenslet_tf(model_opt)\n",
    "    #model = Model()\n",
    "    psf_stack = model_opt.gen_psf_stack(T, aper, 0.)\n",
    "    stack_spectrum_opt = model_opt.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        ax[n].plot((model_init.dc_mask*tf.abs(stack_spectrum_init[n])).numpy()[0,:model.samples[1]//2],'r',label='Initial spectrum z{}'.format(n))\n",
    "        ax[n].plot((model_opt.dc_mask*tf.abs(stack_spectrum_opt[n])).numpy()[0,:model.samples[1]//2],'k',label='Optimized spectrum z{}'.format(n))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Spectrum')\n",
    "        ax[n].set_ylim(top=5)\n",
    "        \n",
    "def plot_spectra_radial(model):\n",
    "    T,aper=make_lenslet_tf(model)\n",
    "    #model = Model()\n",
    "    psf_stack = model.gen_psf_stack(T, aper, 0.)\n",
    "    stack_spectrum = model.gen_stack_spectrum(psf_stack)\n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        radprof = radial_profile((model.dc_mask*tf.abs(stack_spectrum[n])).numpy(),(0,0))\n",
    "        ax[n].plot(radprof[:model.samples[1]//2],'r',label='spectrum z{}'.format(n))\n",
    "        ax[n].plot(model.target_F.numpy()[0,:model.samples[1]//2],'k',label='Target res: {}'.format(model.target_res*100//1))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Spectrum')\n",
    "        ax[n].set_ylim(bottom=0,top=10)\n",
    "\n",
    "        \n",
    "# radial average\n",
    "def compare_spectra_radial(model_init, model_opt):\n",
    "    T,aper=make_lenslet_tf(model_init)\n",
    "    #model = Model()\n",
    "    psf_stack = model_init.gen_psf_stack(T, aper, 0.)\n",
    "    stack_spectrum_init = model_init.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    T,aper=make_lenslet_tf(model_opt)\n",
    "    #model = Model()\n",
    "    psf_stack = model_opt.gen_psf_stack(T, aper, 0.)\n",
    "    stack_spectrum_opt = model_opt.gen_stack_spectrum(psf_stack)\n",
    "    \n",
    "    f, ax = plt.subplots(1,model.Nz,figsize=(25,3))\n",
    "    for n in range(len(ax)):\n",
    "        \n",
    "        radprof_init = radial_profile((model_init.dc_mask*tf.abs(stack_spectrum_init[n])).numpy(),(0,0))\n",
    "        ax[n].plot(radprof_init[:model_init.samples[1]//2],'r',label='Init z{}'.format(n))\n",
    "        #ax[n].plot((model_init.dc_mask*tf.abs(stack_spectrum_init[n])).numpy()[0,:model.samples[1]],'r',label='Initial spectrum z{}'.format(n))\n",
    "        #ax[n].plot((model_opt.dc_mask*tf.abs(stack_spectrum_opt[n])).numpy()[0,:model.samples[1]],'k',label='Optimized spectrum z{}'.format(n))\n",
    "        radprof_opt = radial_profile((model_opt.dc_mask*tf.abs(stack_spectrum_opt[n])).numpy(),(0,0))\n",
    "        ax[n].plot((radprof_opt)[:model_init.samples[1]//2],'k',label='Opt z{}'.format(n))\n",
    "        ax[n].legend()\n",
    "        ax[n].set_title('Radially averaged spectrum')\n",
    "        ax[n].set_ylim(bottom=0)\n",
    "    \n",
    "#model_opt = Model()\n",
    "#model_opt.load_weights('C:\\\\Users\\\\herbtipa\\\\logsumexp_1000_2p5.hd5')\n",
    "plot_spectra_radial(model_init)\n",
    "\n",
    "\n",
    "#re_init_model()\n",
    "plot_spectra_radial(model)\n",
    "compare_spectra_radial(model_init, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ropt = model(0).numpy()\n",
    "xcorr_opt = calc_max_xcorr(model)\n",
    "Rinit = model_noaber(0).numpy()\n",
    "xcorr_init = calc_max_xcorr(model_noaber)\n",
    "\n",
    "f, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(Ropt[:model.Nz], 'rx',label='aberrations')\n",
    "ax[0].plot(Rinit[:model.Nz],  'k.',label='no aberrations')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Spectral fitting')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('z-plane')\n",
    "\n",
    "\n",
    "ax[1].plot(xcorr_opt, 'rx',label='aberrations')\n",
    "ax[1].plot(xcorr_init,  'k.',label='no aberrations')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Cross correlations')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('z-plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Ropt = model(0).numpy()\n",
    "xcorr_opt = calc_max_xcorr(model)\n",
    "Rinit = model_init(0).numpy()\n",
    "xcorr_init = calc_max_xcorr(model_init)\n",
    "\n",
    "f, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(Ropt[:model.Nz], 'rx',label='optimized')\n",
    "ax[0].plot(Rinit[:model.Nz],  'k.',label='init')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Spectral fitting')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('z-plane')\n",
    "\n",
    "\n",
    "ax[1].plot(xcorr_opt, 'rx',label='optimized')\n",
    "ax[1].plot(xcorr_init,  'k.',label='init')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Cross correlations')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('z-plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.target_F*model.dc_mask)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(1,2,figsize=(15,5))\n",
    "test2 = model.gen_psf_stack(T, aper, 0., zplanes =defocus_grid[0])\n",
    "\n",
    "spect=model.gen_stack_spectrum(test2)\n",
    "vrange = 20\n",
    "f1 = ax1[0].imshow((np.abs(spect[-1].numpy()) - 1*np.abs(model.target_F.numpy())),vmax=vrange,vmin=-vrange)\n",
    "fig1.colorbar(f1,ax=ax1[0])\n",
    "\n",
    "test3 = model_init.gen_psf_stack(Tinit, aper,0., zplanes =defocus_grid[0])\n",
    "\n",
    "spect3=model.gen_stack_spectrum(test3)\n",
    "f2 = ax1[1].imshow((np.abs(spect3[-1].numpy()) - 1*np.abs(model.target_F.numpy())),vmax=vrange,vmin=-vrange)\n",
    "fig1.colorbar(f1,ax=ax1[1])\n",
    "print(np.sum(np.abs(np.abs(spect3[-1].numpy()) - 1*np.abs(model.target_F.numpy()))*model.dc_mask*np.abs(model.target_F.numpy()>.001)))\n",
    "print(np.sum(np.abs(np.abs(spect[-1].numpy()) - 1*np.abs(model.target_F.numpy()))*model.dc_mask*np.abs(model.target_F.numpy()>.001)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(model.target_F.numpy())>.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "target_real = tf.ifft2d(tf.complex(model.target_F, 0.))\n",
    "# f2 = ax[0].imshow((tf.real(tf.ifft2d(spect[0]*tf.conj(spect[0]))) - tf_fftshift(model.target_airy))[0:50,0:50])\n",
    "f2 = ax[0].imshow((tf.real(tf.ifft2d(spect[0]*tf.conj(spect[0]))) - 1*(tf.real(target_real) ))[0:50,0:50])\n",
    "f1 = ax[1].imshow((tf.real(target_real))[0:50,0:50])\n",
    "fig.colorbar(f1)\n",
    "fig.colorbar(f2, ax=ax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.target_airy)\n",
    "plt.colorbar()\n",
    "print(tf.reduce_max(model.target_airy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tf.ifft2d(model.target_F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#def plot_psf_stack(model):\n",
    "T,aper,T2=make_lenslet_tf_zern(model)\n",
    "test2 = model.gen_psf_stack(T, aper, 0.5, zplanes = 1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, 20)))\n",
    "#test22 = model.gen_psf_stack(T2, aper, 0, zplanes = 1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, 20)))\n",
    "\n",
    "    #test = model.gen_stack_spectrum(test2)\n",
    "#test3=model.gen_correlation_stack(test)\n",
    "#ax[0].imshow(T)\n",
    "#plt.plot((model.dc_mask * tf.abs(test[1]))[-1,:200].numpy())\n",
    "#plt.plot((model.dc_mask * model.target_F)[-1,:200].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test2[3].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(len(test2)):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    plt.cla()\n",
    "    plt.subplot(1,2,1), plt.imshow(test2[z].numpy())\n",
    "    plt.subplot(1,2,2), plt.imshow(test2[z].numpy())\n",
    "    #plt.plot(np.abs(test3[z][model.samples[0],:]))\n",
    "\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.pause(.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/media/hongdata/Kristina/MiniscopeData/aberrations.hd5',overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_list = 1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, 20)) #mm or dioptres\n",
    "plot_psf_stack(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Video of PSF Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_psf_stack(model):\n",
    "    T,aper, T2=make_lenslet_tf_zern(model)\n",
    "    test2 = model.gen_psf_stack(T, aper, .5, zplanes = 1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, 20)))\n",
    "    test22 = model.gen_psf_stack(T2, aper, .5, zplanes = 1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, 20)))\n",
    "    fig, ax = plt.subplots(1,2,figsize=(15,10))\n",
    "    #test = model.gen_stack_spectrum(test2)\n",
    "    #test3=model.gen_correlation_stack(test)\n",
    "    #ax[0].imshow(T)\n",
    "    #plt.plot((model.dc_mask * tf.abs(test[1]))[-1,:200].numpy())\n",
    "    #plt.plot((model.dc_mask * model.target_F)[-1,:200].numpy())\n",
    "    ims = []\n",
    "    for z in range(len(test2)):\n",
    "        \n",
    "        #fig = plt.figure(figsize=(15,5))\n",
    "        #plt.cla()\n",
    "        #plt.subplot(1,2,1), plt.imshow(test2[z].numpy())\n",
    "        #plt.subplot(1,2,2), plt.imshow(test22[z].numpy())\n",
    "        psf1 = ax[0].imshow(test2[z].numpy(),animated=True)\n",
    "        psf2 = ax[1].imshow(test22[z].numpy(),animated=True)\n",
    "        ax[0].set_xticks([])\n",
    "        ax[0].set_yticks([])\n",
    "        ax[1].set_xticks([])\n",
    "        ax[1].set_yticks([])\n",
    "\n",
    "        #plt.plot(np.abs(test3[z][model.samples[0],:]))\n",
    "\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        ims.append([psf1, psf2])\n",
    "    return ims, fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims, fig = plot_psf_stack(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ani = animation.ArtistAnimation(fig, ims, interval = 100, blit = True, repeat_delay = 100)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_in = tf_exp(model.k*(model.Grin[0]+defocus_grid[0] -defocus_grid[0]*tf.sqrt(1+tf.square(model.ygm/defocus_grid[0])+tf.square(model.xgm/defocus_grid[0])) )) #negative already included\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_out = U_in * tf_exp((model.k*(model.ior-1)*T + 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zernlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def psf_slider(z, psf_stack):\n",
    "    \n",
    "    #test = model.gen_stack_spectrum(test2)\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    plt.imshow(psf_stack[z].numpy())\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_list = 1./(np.linspace(1/model.zmin_virtual, 1./model.zmax_virtual, 15)) #mm or dioptres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, .5, zplanes_opt = z_list)\n",
    "np.shape(test2[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "interact(psf_slider, z = widgets.IntSlider(min=1, max = 15, step=1, value=1), test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_psf_stack(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to run reconstructions in loop (to run many initializations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_opt(iterations, model, ind, CA):\n",
    "    \n",
    "    xpos0 = model.xpos.numpy()\n",
    "    ypos0 = model.ypos.numpy()\n",
    "    rlist0 = model.rlist.numpy()\n",
    "    offset0 = model.lenslet_offset.numpy()\n",
    "    \n",
    "    step_size = 1e-8\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "\n",
    "    losslist=[]\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        num_grads = 10\n",
    "        grad_averaged = 1/num_grads * gradients_and_scaling(model, loss)\n",
    "        for g in range(0,num_grads-1):\n",
    "            grads, lossvalue = gradients_and_scaling(model, loss)\n",
    "        \n",
    "            for grad_ind in range(0,len(model.variables)):\n",
    "                grad_averaged[grad_ind] = grad_averaged[grad_ind] + 1/num_grads * grads[grad_ind]\n",
    "\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads_averaged,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "        project_to_aper_keras(model)\n",
    "        T,aper=make_lenslet_tf(model)\n",
    "\n",
    "        losslist.append(lossvalue.numpy())\n",
    "        \n",
    "    xposf = model.xpos.numpy()\n",
    "    yposf = model.ypos.numpy()\n",
    "    rlistf = model.rlist.numpy()\n",
    "    offsetf = model.lenslet_offset.numpy()    \n",
    "    save_dict = {'xpos0': xpos0,\n",
    "             'ypos0': ypos0,\n",
    "             'rlist0': rlist0,\n",
    "             'offset0': offset0,\n",
    "            'losses': losslist,\n",
    "             'xposf': xposf,\n",
    "             'yposf': yposf,\n",
    "             'rlistf': rlistf,\n",
    "             'offsetf': offsetf,}\n",
    "    \n",
    "    save_extension = 'test_'+ str(CA)+ '_' + str(ind)+'_initial_loss_' + str(losslist[0])+ '_final_loss_' + str(losslist[-1]) + '.mat'\n",
    "    scipy.io.savemat('/home/kyrollos/randoscope/data_1_12_2019/' + save_extension, save_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Re-initialize model\n",
    "model.xpos.assign(xinit)\n",
    "model.ypos.assign(yinit)\n",
    "model.lenslet_offset.assign(offsetinit)\n",
    "model.rlist.assign(rinit)\n",
    "print(model.xpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#[model.variables[n].assign(init_vars) for n in range(nvars)]\n",
    "# Gradient descent\n",
    "step_size = 1e-9\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=step_size)\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "losslist=[]\n",
    "rmean=[]\n",
    "tk = tf.constant(1,tf.float32)\n",
    "tkp = tf.constant(1,tf.float32)\n",
    "# xk = model.variables\n",
    "# xkp = model.variables\n",
    "# nvars = np.shape(xk)[0]\n",
    "for i in range(70):\n",
    "    grad,lossvalue, Rmat=gradient(model,loss_sum)\n",
    "\n",
    "   # new_xpos = model.xpos - step_size*grad[2]\n",
    "   # new_ypos = model.xpos - step_size*grad[3] \n",
    "\n",
    "    #new_grad, test_dist, test_dist_bool = constrain_distances(model, new_xpos, new_ypos, grad) # apply constraint \n",
    "    #print(new_grad)\n",
    "    grad[1] = grad[1] * 100000  #Radius\n",
    "    grad[2]=grad[2]*10000 # X\n",
    "    grad[3]=grad[3]*10000 # Y\n",
    "    #grad[2] = grad[2]*new_grad[0,:]*10000\n",
    "   # grad[3] = grad[3]*new_grad[1,:]*10000   # update the gradient\n",
    "#    grad[1] = grad[1]*\n",
    "    \n",
    "    #grads=remove_nan_gradients(grad)\n",
    "    #print(grads)\n",
    "    \n",
    "    # Gradient step\n",
    "    optimizer.apply_gradients(zip(grad,model.variables),global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    # Projection step\n",
    "    project_to_aper_keras(model)\n",
    "    \n",
    "\n",
    "    \n",
    "    T,aper=make_lenslet_tf(model)\n",
    "\n",
    "    losslist.append(lossvalue)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.cla()\n",
    "    #plt.plot(model.xpos.numpy(),model.ypos.numpy(),'o')\n",
    "    #plt.axis('equal')\n",
    "    #rmean.append(tf.reduce_mean(model.rlist.numpy()))\n",
    "    #plt.plot(rmean)\n",
    "    plt.imshow(T.numpy())\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.cla()\n",
    "    plt.semilogy(losslist)\n",
    "    \n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.cla()\n",
    "\n",
    "    #rshow = plt.imshow(np.tril(Rmat[model.Nz+1:-1])\n",
    "#     cbar = fig.colorbar(rshow)    \n",
    "    pl = plt.plot(Rmat.numpy(),'k.')\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    pl.clear()\n",
    "#     cbar.remove()\n",
    "# cbar = fig.colorbar(rshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,3))\n",
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, .5)\n",
    "test = model.gen_stack_spectrum(test2)\n",
    "test4=model.gen_correlation_stack(test)\n",
    "#ax[0].imshow(T)\n",
    "\n",
    "\n",
    "# for z in range(len(test4)):\n",
    "\n",
    "#     #ax[1].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z])))), vmin = 0, vmax = 50)\n",
    "#     #ax[2].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z]))) - model.target_F), vmin = -50, vmax = 50)\n",
    "#     plt.cla()\n",
    "#     #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))), vmax = 50)\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.plot(np.abs(test3[z][model.samples[0],:]))\n",
    "    \n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.plot(np.abs(test4[z][model.samples[0],:]))\n",
    "\n",
    "#     display.display(fig)\n",
    "#     display.clear_output(wait=True)\n",
    "#     plt.pause(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "for z in range(7):\n",
    "\n",
    "    #ax[1].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z])))), vmin = 0, vmax = 50)\n",
    "    #ax[2].imshow(tf_fftshift(tf.abs((test[z] * tf.conj(test[z]))) - model.target_F), vmin = -50, vmax = 50)\n",
    "    #plt.cla()\n",
    "    #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))), vmax = 50)\n",
    "    plt.subplot(7,2,z*2+1)\n",
    "    ind = (model.Nz+1) * (z)\n",
    "    plt.plot(np.abs(test3[ind][model.samples[0],:]))\n",
    "    \n",
    "    plt.subplot(7,2,z*2+2)\n",
    "    plt.plot(np.abs(test4[ind][model.samples[0],:]))\n",
    "\n",
    "    #display.display(fig)\n",
    "    #display.clear_output(wait=True)\n",
    "    #plt.pause(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(tf.reshape(model(0),(model.Nz, model.Nz)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.sort(1/model.rlist.numpy()),'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.lenslet_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T,aper=make_lenslet_tf(model)\n",
    "test2 = model.gen_psf_stack(T, aper, 0.5)\n",
    "test = model.gen_stack_spectrum(test2)\n",
    "fig = plt.figure()\n",
    "\n",
    "f,ax = plt.subplots(1,2,figsize=(20,20))\n",
    "\n",
    "ax[0].imshow(T)\n",
    "\n",
    "\n",
    "for z in range(model.Nz):\n",
    "    #ax[1].imshow((tf.real((test[z] * tf.conj(test[z])))) - model.target_F, vmax = 50)\n",
    "    im_disp = ax[1].imshow((tf.real((test2[z]))),vmax = .3)\n",
    "    cbar = plt.colorbar(im_disp, ax=ax[1])\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    cbar.remove()\n",
    "    #plt.pause(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def animate_zstack(i):\n",
    "#    ax[1].clear()\n",
    "    ax[1].imshow(tf.real(test2[i]),vmax = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.save_weights('C:\\\\Users\\\\herbtipa\\\\lenslets_one_per_depth.hd5',overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(20,20))\n",
    "ax[0].imshow(T)\n",
    "ani = animation.FuncAnimation(f, animate_zstack, frames = range(model.Nz), interval = 500, repeat = False)\n",
    "# plt.show(ani)\n",
    "#Writer = animation.writers['ffmpeg']\n",
    "\n",
    "# HTML(ani.to_html5_video())\n",
    "for i in range(model.Nz):\n",
    "    animate_zstack(i)\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    f.savefig('C:\\\\Users\\\\herbtipa\\\\foo_{}.png'.format(i))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#jacobian=[]\n",
    "#for i in range (model.Nz**2):\n",
    "#    with tf.GradientTape() as tape:\n",
    "#        #tape.watch(model.variables)\n",
    "#        R=model(0)\n",
    "#        jacobian.append(tape.gradient(R[i], model.variables))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "eager-latest2",
   "language": "python",
   "name": "homekyrollosanaconda3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
